{"cells":[{"metadata":{},"cell_type":"markdown","source":"* Course    : Artificial Intelligence (CS6613) : New York University\n* Project 1 : Surface Type Classification\n* Author    : Harsh Thakkar (ht1215)\n\n***Problem statement*** : Given the sensor measurement in the form of orientation values, angular velocity and linear acceleration values on different surfaces. Train machine learning model to predict the surface for separately given test data.\n\n***Overview of the data*** : \nTraining data (X) contains 3790 measurements each having 128 different sample reading and (y) contains 3790 rows denoting Surface type for the given series of measurements\n\nTesting data contains 20 different measurements each having 128 sample reainds and (y) contains 20 rows denoting Surface Type\n\nPandas library is used to read data from csv files and store in the variables which will later be used for training and testing purposes"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\nx_train = pd.read_csv('/kaggle/input/career-con-2019/X_train.csv')\ny_train = pd.read_csv('/kaggle/input/career-con-2019/y_train.csv')\n\nx_new_train = pd.read_csv('/kaggle/input/new-dataset/X_train_new.csv')\ny_new_train = pd.read_csv('/kaggle/input/new-dataset/y_train_new.csv')\n\nx_test = pd.read_csv('/kaggle/input/new-dataset/X_test_new.csv')\ny_test = pd.read_csv('/kaggle/input/new-dataset/y_test_new.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_new_train.shape, x_test.shape, y_new_train.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the given dataset, we have total 9 different Surface types on which the robot has collected sensor data and that is stored in string format. To be used the data in machine learning model, it is computationally more convenient to convert the string format into corresponding integer format which is easy to compute and to convert back to string format when the operation is performed. Label binarizer and label encoder can be used to convert labels into integers. Label binarizer creates seperate copy for each input and stores as binary value where as label encoder classifies into number of different labels available. \n\nHere, y_new_train['surface'] column is labelised using label binariser and it can be seen that it stores binary values in matrix format where single row has one entry as 1 denoting the presence of that class. Numpy library is used to convert it into numpy array."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelBinarizer, LabelEncoder\nimport numpy as np\nlb = LabelBinarizer()\nle = LabelEncoder()\n\nnew_target = lb.fit_transform(y_new_train['surface'])\nnew_target = np.array(new_target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_target","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**DATA VISUALIZATION and DATA DEPENDANCY**\nSeaborn library is very useful visualizing library. Countplot is used to plot number of different samples for each surface type. For example, there are 792 sample measurements for concrete surface, 310 samples for hard tiles large space surface. \n\nHeatmap helps in visualizing different dependancies between sensor values and gives idea about how they are proportional to each other."},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nfrom seaborn import countplot\nimport matplotlib.pyplot as plt\ncountplot(y='surface', data = y_train)\nplt.show()\n\nf, ax = plt.subplots(figsize= (8,8))\nsns.heatmap(x_train.iloc[:,3:].corr(), annot = True, linewidth = .5, fmt = '.1f', ax = ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This section of code represents different sensor values concentrations for different surface types. This helps in differentiating and analysing what sensor values will be useful features for differentiating between different surface types"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_feature_class_distribution(classes,tt, features,a=5,b=2):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(a,b,figsize=(22,36))\n\n    for feature in features:\n        i += 1\n        plt.subplot(a,b,i)\n        for clas in classes:\n            ttc = tt[tt['surface']==clas]\n            sns.kdeplot(ttc[feature], bw=0.5,label=clas)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=8)\n        plt.tick_params(axis='y', which='major', labelsize=8)\n    plt.show();\n\nfeatures = x_train.columns.values[3:]\nclasses = (y_train['surface'].value_counts()).index\naux = x_train.merge(y_train, on='series_id', how='inner')\nplot_feature_class_distribution(classes, aux, features, a = int (features.shape[0]/2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Gimbal lock : Euler measurement can create gimbal lock when 2 of the axis aling in the same plane \nand in this situation rotating those 2 axis in the same plane can no longer move the object in any\nother direction. That is why we are given data in quaternion formation and we can convert quaternion to euler.\n\nReference : [wiki](https://en.wikipedia.org/wiki/Conversion_between_quaternions_and_Euler_angles)\n\n[youtube](https://www.youtube.com/watch?v=zc8b2Jo7mno)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def quaternion_to_euler(x, y, z, w):\n    import math\n    t0 = +2.0 * (w * x + y * z)\n    t1 = +1.0 - 2.0 * (x * x + y * y)\n    X = math.atan2(t0, t1)\n    \n    t2 = +2.0 * (w * y - z * x)\n    t2 = +1.0 if t2 > +1.0 else t2\n    t2 = -1.0 if t2 < -1.0 else t2\n    Y = math.asin(t2)\n\n    t3 = +2.0 * (w * z + x * y)\n    t4 = +1.0 - 2.0 * (y * y + z * z)\n    Z = math.atan2(t3, t4)\n\n    return X, Y, Z\n\ndef feature_extraction (data):\n    data['norm_orientation'] = ((data['orientation_X']**2 + data['orientation_Y']**2 + data['orientation_Z']**2 + data['orientation_W']**2)**0.5)\n    data['norm_X'] = data['orientation_X'] / data['norm_orientation']\n    data['norm_Y'] = data['orientation_Y'] / data['norm_orientation']\n    data['norm_Z'] = data['orientation_Z'] / data['norm_orientation']\n    data['norm_W'] = data['orientation_W'] / data['norm_orientation'] \n    data['ang_y_vs_z'] = data['angular_velocity_Y'] * data['angular_velocity_Z']\n    data['norm_angular'] = ((data['angular_velocity_X']**2 + data['angular_velocity_Y']**2 + data['angular_velocity_Z']**2)**0.5)\n    data['norm_linear'] = ((data['linear_acceleration_X']**2 + data['linear_acceleration_Y']**2 + data['linear_acceleration_Z']**2)**0.5)\n    data['acc_vs_velocity'] = data['norm_angular'] / data['norm_linear']\n    x, y, z, w = data['norm_X'].tolist(), data['norm_Y'].tolist(), data['norm_Z'].tolist(), data['norm_W'].tolist()\n    nx, ny, nz = [], [], []\n    for i in range(len(x)):\n        xx, yy, zz = quaternion_to_euler(x[i], y[i], z[i], w[i])\n        nx.append(xx)\n        ny.append(yy)\n        nz.append(zz)\n    data['euler_x'] = nx\n    data['euler_y'] = ny\n    data['euler_z'] = nz\n    return data\n\nx_train = feature_extraction(x_train)\nx_new_train = feature_extraction(x_new_train)\nx_test = feature_extraction(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = x_new_train.columns.values[4:]\nclasses = (y_new_train['surface'].value_counts()).index\naux = x_new_train.merge(y_new_train, on='series_id', how='inner')\nplot_feature_class_distribution(classes, aux, features, a = int (features.shape[0]/2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By looking at the graphs above, Some of the columns actually show variation for different surface types\nand those features can be used to train the model. Below are the new data frame with the only important \nfeatures extracting from above data frame"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = x_train[['series_id', 'norm_X', 'norm_Y', 'norm_Z', 'norm_W','linear_acceleration_X', 'linear_acceleration_Y', 'linear_acceleration_Z', 'angular_velocity_X', 'angular_velocity_Y', 'angular_velocity_Z']]\nx_new_train = x_new_train[['norm_X', 'norm_Y', 'norm_Z', 'norm_W','linear_acceleration_X', 'linear_acceleration_Y', 'linear_acceleration_Z', 'angular_velocity_X', 'angular_velocity_Y', 'angular_velocity_Z']]\nx_test = x_test[['norm_X', 'norm_Y', 'norm_Z', 'norm_W','linear_acceleration_X', 'linear_acceleration_Y', 'linear_acceleration_Z', 'angular_velocity_X', 'angular_velocity_Y', 'angular_velocity_Z']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As there are 128 samples for different measurements, this function combines and reshapes the resultant array in 3 dimensional array with (samples, measurement count, total features)\n\nTraining sample then becomes of the dimension (3790, 128, 10) and testing sample becomes of the dimension of (20, 128, 10)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def combine_feature(train):\n    segments = []\n\n    for i in range(0, len(train), 128): \n        ox = train.norm_X.values[i:i+128]\n        oy = train.norm_Y.values[i:i+128]\n        oz = train.norm_Z.values[i:i+128]\n        ow = train.norm_W.values[i:i+128]\n        ax = train.angular_velocity_X.values[i:i+128]\n        ay = train.angular_velocity_Y.values[i:i+128]\n        az = train.angular_velocity_Z.values[i:i+128]\n        lx = train.linear_acceleration_X.values[i:i+128]\n        ly = train.linear_acceleration_Y.values[i:i+128]\n        lz = train.linear_acceleration_Z.values[i:i+128]\n        \n        segments.append([ox, oy, oz, ow, ax, ay,az,lx, ly, lz])\n    segments = np.asarray(segments, dtype= np.float32).reshape(-1, 128, 10)\n    return segments","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_train = combine_feature(x_new_train)\ntest = combine_feature(x_test)\nnew_train.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Convolution neural network** is widely used in Image processing and feature extraction known as 2D CNN in which kernel moves in two dimentions or 3 dimensions and learns important spatial features. In this question, given dataset contains time-series data in which 128 different sensor measurements are given. In 1D CNN network, kernel can move in only 1 direction that is in the direction of the time/samples. \n\n\nInput to the CNN model is, matrix of size (128, 10) where 128 is the sample size and 10 is the number of features. First 1D CNN maps all sensors values to 128 values and this it does for all 128 time samples and creates matrix of 128,128.\n\nNext 1D CNN layer uses 64 filters for 64 different sampled features and with kernel size (3,1). This layer learns how single feature depends on each other over time. \n\nTOTAL NUMBER OF PARAMETERS USED : 73,753\nAVG TRAINING ACCURACY : 0.85\nAVG VALIDATION ACCURACY : 0.75"},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Activation, MaxPooling1D\nfrom keras.layers import Conv1D","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def my_cnn():\n    model = Sequential()\n    \n    model.add(Conv1D(filters = 128, kernel_size = 1, input_shape = (128,10), activation = 'relu'))\n    model.add(MaxPooling1D(pool_size=(2)))\n    model.add(Dropout(0.1))\n\n    model.add(Conv1D(filters = 64, kernel_size = 3, activation = 'relu'))\n    model.add(MaxPooling1D(pool_size=(2)))\n    model.add(Dropout(0.1))\n\n    model.add(Conv1D(filters = 32,kernel_size = 5, activation = 'relu'))\n    model.add(Conv1D(filters = 32,kernel_size = 5, activation = 'relu'))\n    model.add(MaxPooling1D(pool_size=(2)))\n    model.add(Dropout(0.1))\n\n    model.add(Conv1D(filters = 16,kernel_size = 5, activation = 'relu'))\n    model.add(MaxPooling1D(pool_size=(2)))\n    model.add(Dropout(0.1))\n     \n    model.add(Flatten())\n    model.add(Dense(512))\n    model.add(Dropout(0.1))\n    model.add(Dense(9, activation='softmax'))\n    \n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = my_cnn()\nfilepath = \"skynet.hdf5\"\ncallbacks_list = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=10), \n                 keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)]\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(new_train, new_target, batch_size = 100, epochs = 200, callbacks=callbacks_list, validation_split = 0.1, shuffle = True, verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = model.predict(test)\nprediction","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**TESTING AGAINST DIFFERENT MACHINE LEARNING MODELS (LINEAR REGRESSION, SVM, KNeighbourclassifier) :**\n\nIn the main dataset, below function merges all the columns for the same measurement and find the values according to the operation. Like, it merges all the column and finds the maximum value out of all the values. Some of the useful functions have been used to extract feature information for different sensor data. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def SSC(x):\n    x = np.array(x)\n    x = np.append(x[-1], x)\n    x = np.append(x,x[1])\n    xn = x[1:len(x)-1]\n    xn_i2 = x[2:len(x)]    # xn+1 \n    xn_i1 = x[0:len(x)-2]  # xn-1\n    ans = np.heaviside((xn-xn_i1)*(xn-xn_i2),0)\n    return sum(ans[1:])\n\ndef SRAV(x):    \n    SRA = sum(np.sqrt(abs(x)))\n    return np.power(SRA/len(x),2)\n\ndef _kurtosis(x):\n    return kurtosis(x)\n\ndef feature_transformation(data):\n    data_feature = pd.DataFrame()\n    \n    for col in data.columns:\n        if col in ['row_id','series_id','measurement_number']:\n            continue   \n        data_feature[col + '_skew'] = data.groupby(['series_id'])[col].skew()\n        data_feature[col + '_mad'] = data.groupby(['series_id'])[col].mad()\n        data_feature[col + '_q70'] = data.groupby(['series_id'])[col].quantile(0.7)\n        data_feature[col + '_q90'] = data.groupby(['series_id'])[col].quantile(0.9)\n        data_feature[col + '_SSC'] = data.groupby(['series_id'])[col].apply(SSC) \n        data_feature[col + '_SRAV'] = data.groupby(['series_id'])[col].apply(SRAV)\n        data_feature[col + '_mean'] = data.groupby(['series_id'])[col].mean()\n        data_feature[col + '_median'] = data.groupby(['series_id'])[col].median()\n        data_feature[col + '_max'] = data.groupby(['series_id'])[col].max()\n        data_feature[col + '_min'] = data.groupby(['series_id'])[col].min()\n        data_feature[col + '_std'] = data.groupby(['series_id'])[col].std()\n        data_feature[col + '_range'] = data_feature[col + '_max'] - data_feature[col + '_min']\n        data_feature[col + '_maxtoMin'] = data_feature[col + '_max'] / data_feature[col + '_min']\n    return data_feature\n\ntrain_feature = feature_transformation(x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nfrom scipy.stats import kurtosis\nfrom scipy.stats import skew\n\ntrain_feature.fillna(0,inplace=True)\ntrain_feature.replace(-np.inf,0,inplace=True)\ntrain_feature.replace(-np.inf,0,inplace=True)\n\n#predicted = np.zeros((test_feature.shape[0],9))\nmeasured= np.zeros((train_feature.shape[0]))\nscore = 0\nimport gc\ngc.enable()\n\ny_train['surface'] = le.fit_transform(y_train['surface'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Linear regression**\nKnowing the relationship between dependent variable and independent variable, linear regression can help in finding the value of the label which depends on the sensors features. Linear regression tries to find the parameters in such a way that it minimises the mean square error of the model. It uses hyper dimensional linear equation Y = \\theta*X + b where X is the matrix of the feature. At the end, it predicts the values and tries to minimises the MSE and updates the parameter values.\n\nAVG ACCURACY FOR 100 FOLDS : 0.2"},{"metadata":{"trusted":true},"cell_type":"code","source":"folds = StratifiedKFold(n_splits=100, shuffle=True, random_state=1234)\n\nfrom sklearn.linear_model import LinearRegression\ndef linearRegression(measured, score, train_feature, y_train):\n    for times, (t_idx, v_idx) in enumerate(folds.split(train_feature.values, y_train['surface'].values)):\n        model = LinearRegression().fit(train_feature.iloc[t_idx], y_train['surface'][t_idx])\n        measured[v_idx] = model.predict(train_feature.iloc[v_idx])\n        score += model.score(train_feature.iloc[v_idx], y_train['surface'][v_idx])\n        print(\"Fold: {} score: {}\".format(times,model.score(train_feature.iloc[v_idx],y_train['surface'][v_idx])))    \n        gc.collect()\n\nprint(\"Linear regression\")\nlinearRegression(measured, score, train_feature, y_train)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**KNearestNeighbour Algorithm**\n\nA data is assinged to class group to which it is most closed in terms of euclidean distance. \n\nAVERAGE ACCURACY : 0.6"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\ndef KNeighborsClassifier_():\n    score = 0\n    for times, (trn_idx, val_idx) in enumerate(folds.split(train_feature.values, y_train['surface'].values)):\n        model = KNeighborsClassifier().fit(train_feature.iloc[trn_idx], y_train['surface'][trn_idx])\n        measured[val_idx] = model.predict(train_feature.iloc[val_idx])\n        score += model.score(train_feature.iloc[val_idx], y_train['surface'][val_idx])\n        print(\"Fold: {} score: {}\".format(times,model.score(train_feature.iloc[val_idx],y_train['surface'][val_idx])))    \n        gc.collect()\n\n\n\nprint(\"KNeighboursclassifiers\")\nKNeighborsClassifier_(),","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Support Vector Machines**\nAfter observing in linear regression that the data seems like linearly inseperable, support vector machine's polynomial and rbf kernels can be used to effectively classify multiclass. It seperates hyper plane with the maximum margin. \n\nAVERAGE TRAINING ACCURACY : 0.4"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\ndef svm_model(train_feature,  y_train):\n    for times, (trn_idx, val_idx) in enumerate(folds.split(train_feature.values,y_train['surface'].values)):\n        model = SVC(kernel= 'rbf', C = 1)\n        model.fit(train_feature.iloc[trn_idx],y_train['surface'][trn_idx])\n        measured[val_idx] = model.predict(train_feature.iloc[val_idx])\n        print(\"Fold: {} score: {}\".format(times,model.score(train_feature.iloc[val_idx],y_train['surface'][val_idx])))\n        gc.collect()\n\nprint(\"Support vector machine model\")\nsvm_model(train_feature, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Trained CNN model is able to predict correct label given the data with the accuracy of 75% for the given test dataset in which 20 samples are fed as testing parameters. "},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_y = prediction.argmax(axis = 1)\npred_class = le.inverse_transform(pred_y)\nnum_correct = 0\n\nfor i in range(y_test.shape[0]) :\n    if pred_class[i] == y_test['surface'][i] :\n        num_correct += 1\n        \nprint(\"Score on Testing Data =\", num_correct / y_test.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"answer = pd.DataFrame(pred_class, columns = ['Surface'])\nanswer.to_csv('submission_final.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"answer","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":4}