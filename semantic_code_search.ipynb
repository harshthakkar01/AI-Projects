{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AIProject3.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGJmVhI01wV0"
      },
      "source": [
        "**Course** : Artificial Intelligence\n",
        "\n",
        "\n",
        "**Project** : Semantic code search\n",
        "\n",
        "**Author** : Harsh Thakkar (ht1215)\n",
        "\n",
        "Deep learning models have significantly improved performance and accuracy in terms of image processing and many other machine learning applications and Natural language processing tasks. Still when it comes to semantic code search, it is still not very promising. In this project, I have tried to explain and implement method discussed in the [paper](https://arxiv.org/pdf/1909.09436.pdf). \n",
        "\n",
        "**Dataset**\n",
        "\n",
        "They have collected corpus from publicly available open-source GITHUB and identified repositories that have been used by atleast one projects and sorted them by popularity in terms of stars and forks. Then after removing projects not having license and re-distribution permit, they combined data for python, java, javascript, Go, PHP, Ruby. \n",
        "\n",
        "For filtering, only those files that have documentation associated with them are considered. Pairs of (c_{i}, d_{i}) are formed. Then for pre-processing certain measures are taken. Like documentation is truncated to the first full paragraph and the documentation whose length is less than 3 tokens are removed and functions having length less than 3 are removed. The filtered corpus and data extraction code can be found [here](https://github.com/github/CodeSearchNet) \n",
        "\n",
        "This dataset contains 2 million function-document pairs. \n",
        "\n",
        "\n",
        "\n",
        "**Baseline models**\n",
        "\n",
        "1) Joint vector representation for code search \n",
        "\n",
        "They use embedding of words to create neural search systems. Given natural language and code snippet, the goal is to create joint vector space using these data and make sure that corresponding documentation data is near to the code. For a given query we can then find the embedded vector and try to find which code snippets are near to that query. \n",
        "\n",
        "\n",
        "**Running baseline models**\n",
        "\n",
        "REQUIREMENT : This project assumes you have Docker and Nvidia-Docker, as well as GPU that supports CUDA 9.0 or greater. In this project, I have used paperspace virtual machine to train the model. \n",
        "\n",
        "git clone https://github.com/github/CodeSearchNet.git\n",
        "\n",
        "cd CodeSearchNet/\n",
        "\n",
        "script/setup. # Download the data from S3 (3.5 GB)\n",
        "\n",
        "script/console # Go inside docker container\n",
        "\n",
        "wandb/login # Track your submission and for submitting benchmarks\n",
        "\n",
        "python train.py --help # See all the options available\n",
        "\n",
        "python train.py --model neuralbow\n",
        "\n",
        "python train.py --model 1dcnn /trained_models ../resources/data/python/final/jsonl/train ../resources/data/python/final/jsonl/valid ../resources/data/python/final/jsonl/test # To train the model with python dataset\n",
        "\n",
        "Progress of the train can be seen in Weights and baised dashboard\n",
        "\n",
        "Run - path : piby2/CodeSearchNet/4g5yqg94\n",
        "\n",
        "[Graph](https://drive.google.com/open?id=1xAxdMM9JsvPsyeCS7juyN44iH4VAQtmv)\n",
        "\n",
        "[Public link for Weights and Baised](https://app.wandb.ai/piby2/CodeSearchNet/runs/4g5yqg94/overview?workspace=user-piby2)\n",
        "\n",
        "python predict.py -r piby2/CodeSearchNet/4g5yqg94 # To generate model_prediction.csv file for benchmarking\n",
        "\n",
        "I tried on submitting these files for benchmarking, but I got this error in the predict file. [Error](https://drive.google.com/open?id=1yrCA-6k1zH_B8c-QownP4WpH2AFgZnmX) \n",
        "\n",
        "So, I couldn't submit it for benchmarking in given time frame but I will work on it during the break. Any feedback or suggestion on this one will be greatly appreciated. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SMcGWs2B0Ck"
      },
      "source": [
        ""
      ]
    }
  ]
}